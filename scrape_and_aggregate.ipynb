{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping, Inserting, and Aggregating Hockey Data\n",
    "\n",
    "This notebook demonstrates how to scrape player data from Spotrac, insert the scraped data into a database, and then aggregate the data for further analysis.\n",
    "\n",
    "## 1. Setup and Environment Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    BigInteger,\n",
    "    Column,\n",
    "    Float,\n",
    "    Integer,\n",
    "    MetaData,\n",
    "    String,\n",
    "    Table,\n",
    "    create_engine,\n",
    ")\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the database connection\n",
    "DATABASE_TYPE = os.getenv(\"DATABASE_TYPE\")\n",
    "DBAPI = os.getenv(\"DBAPI\")\n",
    "ENDPOINT = os.getenv(\"ENDPOINT\")\n",
    "USER = os.getenv(\"USER\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "PORT = int(os.getenv(\"PORT\", 5432))\n",
    "DATABASE = os.getenv(\"DATABASE\")\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\"\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping Player Data from Spotrac\n",
    "\n",
    "We will scrape player cap hit data for the years 2015, 2016, and 2017 from the Spotrac website using Selenium and BeautifulSoup.\n",
    "The selenium driver is set to safari browser. If you are using a different browser, you will need to adjust the code. \n",
    "driver = webdriver.Safari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Base URL to scrape\n",
    "BASE_URL = \"https://www.spotrac.com/nhl/rankings/player/_/year/{}/sort/cap_total\"\n",
    "\n",
    "# Years to scrape\n",
    "years = [2015, 2016, 2017]\n",
    "\n",
    "# Directory to store CSV files\n",
    "OUTPUT_DIR = \"player_cap_hits\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dictionary to store DataFrames for each year\n",
    "dfs_by_year = {}\n",
    "\n",
    "def split_player_name(name):\n",
    "    \"\"\"Function to clean and split player name\"\"\"\n",
    "    name_parts = name.split()\n",
    "    first_name = name_parts[0]\n",
    "    last_name = \" \".join(name_parts[1:]) if len(name_parts) > 1 else \"\"\n",
    "    return first_name, last_name\n",
    "\n",
    "# Loop through each year and scrape the data\n",
    "for year in years:\n",
    "    url = BASE_URL.format(year)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until the table is loaded\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"list-group-item\"))\n",
    "    )\n",
    "\n",
    "    # Scroll to the bottom of the page to load all content (if applicable)\n",
    "    while True:\n",
    "        previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(2)  # Wait for new data to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == previous_height:\n",
    "            break  # Exit the loop when no more new content is loaded\n",
    "\n",
    "    # Get page source and parse with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find the elements containing the player names and cap hits\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    cap_hits = []\n",
    "\n",
    "    for item in soup.find_all(\"li\", class_=\"list-group-item\"):\n",
    "        name_div = item.find(\"div\", class_=\"link\")\n",
    "        cap_hit_span = item.find(\"span\", class_=\"medium\")\n",
    "        if name_div and cap_hit_span:\n",
    "            name = name_div.text.strip()\n",
    "            first_name, last_name = split_player_name(name)\n",
    "            cap_hit = cap_hit_span.text.strip()\n",
    "            first_names.append(first_name)\n",
    "            last_names.append(last_name)\n",
    "            cap_hits.append(cap_hit)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df = pd.DataFrame(\n",
    "        {\"firstName\": first_names, \"lastName\": last_names, \"capHit\": cap_hits}\n",
    "    )\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    dfs_by_year[year] = df\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrames for each year\n",
    "for year, df in dfs_by_year.items():\n",
    "    print(f\"Data for {year}:\")\n",
    "    display(df.head())\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "for year, df in dfs_by_year.items():\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"player_cap_hits_{year}.csv\")\n",
    "    df.to_csv(csv_path, index=False, mode=\"w\")\n",
    "\n",
    "print(f\"Data saved to {OUTPUT_DIR} directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inserting Scraped Data into the Database\n",
    "\n",
    "Next, we will insert the scraped data from the CSV files into the `hockey_stats` database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create cap hit tables\n",
    "def create_caphit_table(table_name):\n",
    "    \"\"\"Define table creation function to avoid repetition\"\"\"\n",
    "    return Table(\n",
    "        table_name,\n",
    "        metadata,\n",
    "        Column(\"firstName\", String(50)),\n",
    "        Column(\"lastName\", String(50)),\n",
    "        Column(\"capHit\", String(50)),\n",
    "    )\n",
    "\n",
    "# Create tables for each season\n",
    "metadata = MetaData()\n",
    "seasons = [\"20152016\", \"20162017\", \"20172018\"]\n",
    "tables = {season: create_caphit_table(f\"player_cap_hit_{season}\") for season in seasons}\n",
    "metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "def insert_data_from_csv(engine, table_name, file_path):\n",
    "    \"\"\"Insert data from CSV into the specified database table.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.to_sql(table_name, con=engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"Data inserted successfully into {table_name}\")\n",
    "\n",
    "        # Remove the file after successful insertion\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} deleted successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {file_path} - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing file '{file_path}': {e}\")\n",
    "\n",
    "# Define directories and mappings for insertion\n",
    "csv_files_and_mappings = [\n",
    "    (\"player_cap_hits/player_cap_hits_2015.csv\", \"player_cap_hit_20152016\"),\n",
    "    (\"player_cap_hits/player_cap_hits_2016.csv\", \"player_cap_hit_20162017\"),\n",
    "    (\"player_cap_hits/player_cap_hits_2017.csv\", \"player_cap_hit_20172018\"),\n",
    "]\n",
    "\n",
    "# Insert data into database tables\n",
    "with Session() as session:\n",
    "    for file_path, table_name in csv_files_and_mappings:\n",
    "        insert_data_from_csv(engine, table_name, file_path)\n",
    "\n",
    "    print(\"Data inserted successfully into all tables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate Data and insert into the Database\n",
    "\n",
    "Finally, we will aggregate the data per season per player and insert the results into an aggregated table in the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get data from the database\n",
    "def get_data_from_db(query):\n",
    "    \"\"\"Function to get data from the database.\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        return pd.read_sql(query, connection)\n",
    "\n",
    "def create_aggregated_table(table_name):\n",
    "    \"\"\"Create table schema for aggregated table.\"\"\"\n",
    "    metadata = MetaData()\n",
    "    Table(\n",
    "        table_name,\n",
    "        metadata,\n",
    "        Column(\"player_id\", BigInteger, primary_key=True),\n",
    "        Column(\"firstName\", String),\n",
    "        Column(\"lastName\", String),\n",
    "        Column(\"corsi_for\", Float),\n",
    "        Column(\"corsi_against\", Float),\n",
    "        Column(\"corsi\", Float),\n",
    "        Column(\"CF_Percent\", Float),\n",
    "        Column(\"timeOnIce\", Float),\n",
    "        Column(\"game_count\", Integer),\n",
    "        Column(\"Cap_Hit\", Float),\n",
    "    )\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "# Loop through each season to aggregate data\n",
    "for season in [\"20152016\", \"20162017\", \"20172018\"]:\n",
    "    CORSI_QUERY = f\"SELECT * FROM raw_corsi_{season}\"\n",
    "    df_corsi = get_data_from_db(CORSI_QUERY)\n",
    "    if \"Unnamed: 0\" in df_corsi.columns:\n",
    "        df_corsi = df_corsi.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    GSS_TOI_QUERY = 'SELECT game_id, player_id, \"timeOnIce\" FROM game_skater_stats'\n",
    "    df_gss_toi = get_data_from_db(GSS_TOI_QUERY)\n",
    "\n",
    "    PLAYER_INFO_QUERY = (\n",
    "        'SELECT player_id, \"firstName\", \"lastName\", \"primaryPosition\" FROM player_info'\n",
    "    )\n",
    "    df_player_info = get_data_from_db(PLAYER_INFO_QUERY)\n",
    "\n",
    "    # Merge dataframes\n",
    "    df_all = pd.merge(df_corsi, df_gss_toi, on=[\"game_id\", \"player_id\"])\n",
    "    df_all = pd.merge(df_all, df_player_info, on=\"player_id\")\n",
    "\n",
    "    # Group and aggregate player stats\n",
    "    df_grouped_all = (\n",
    "        df_all.groupby(\"player_id\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"firstName\": \"first\",\n",
    "                \"lastName\": \"first\",\n",
    "                \"corsi_for\": \"mean\",\n",
    "                \"corsi_against\": \"mean\",\n",
    "                \"corsi\": \"mean\",\n",
    "                \"CF_Percent\": \"mean\",\n",
    "                \"timeOnIce\": \"mean\",\n",
    "                \"game_id\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"game_id\": \"game_count\"})\n",
    "    )\n",
    "\n",
    "    PLAYER_SALARY_QUERY = (\n",
    "        f'SELECT \"firstName\", \"lastName\", \"capHit\" FROM player_cap_hit_{season}'\n",
    "    )\n",
    "    df_player_salary = get_data_from_db(PLAYER_SALARY_QUERY)\n",
    "    print(df_player_salary.head())\n",
    "\n",
    "    # Convert capHit from string to float\n",
    "    df_player_salary[\"capHit\"] = (\n",
    "        df_player_salary[\"capHit\"].replace(r\"[\\$,]\", \"\", regex=True).astype(float)\n",
    "    )\n",
    "\n",
    "    # Merge aggregated stats with salary info\n",
    "    df_grouped_all = pd.merge(\n",
    "        df_grouped_all, df_player_salary, on=[\"firstName\", \"lastName\"]\n",
    "    )\n",
    "\n",
    "    # Post-processing\n",
    "    df_grouped_all[\"CF_Percent\"] = (df_grouped_all[\"CF_Percent\"].round(4) * 100).round(4)\n",
    "    df_grouped_all[\"timeOnIce\"] = df_grouped_all[\"timeOnIce\"].round(2)\n",
    "\n",
    "    THRESHOLD = 82 * 0.32\n",
    "    df_grouped_all = df_grouped_all.query(f\"game_count >= {THRESHOLD}\")\n",
    "\n",
    "    df_grouped_all[\"CF_Percent\"] = df_grouped_all[\"CF_Percent\"].apply(\n",
    "        lambda x: np.round(x, 4)\n",
    "    )\n",
    "    df_grouped_all[\"timeOnIce\"] = df_grouped_all[\"timeOnIce\"].apply(\n",
    "        lambda x: np.round(x, 2)\n",
    "    )\n",
    "\n",
    "    df_grouped_all = df_grouped_all.sort_values(\"CF_Percent\", ascending=False)\n",
    "\n",
    "    aggregated_table_name = f\"aggregated_corsi_{season}\"\n",
    "    create_aggregated_table(aggregated_table_name)\n",
    "\n",
    "    # Insert aggregated data into the new table\n",
    "    df_grouped_all.to_sql(\n",
    "        aggregated_table_name, con=engine, if_exists=\"replace\", index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Data inserted successfully into {aggregated_table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another database table that will be used for graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Set up the Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Base URL to scrape\n",
    "BASE_URL = \"https://www.spotrac.com/nhl/cap/_/year/{}/sort/cap_maximum_space2\"\n",
    "\n",
    "# Years to scrape\n",
    "years = [2015, 2016, 2017]\n",
    "\n",
    "# Directory to store CSV files\n",
    "OUTPUT_DIR = \"team_salaries\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through each year and scrape the data\n",
    "for year in years:\n",
    "    url = BASE_URL.format(year)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until the table is loaded\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"tablesorter-headerRow\"))\n",
    "    )\n",
    "\n",
    "    # Scroll to the bottom of the page to load all content (if applicable)\n",
    "    while True:\n",
    "        previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(2)  # Wait for new data to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == previous_height:\n",
    "            break  # Exit the loop when no more new content is loaded\n",
    "\n",
    "    # Get page source and parse with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find all rows in the table body\n",
    "    rows = soup.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "    # Find the elements containing the team name and total player payroll\n",
    "    team_names = []\n",
    "    team_payroll = []\n",
    "\n",
    "    # Iterate over each row to extract the name and payroll value\n",
    "    for row in rows:\n",
    "        # Get the team name, usually from the 'a' tag within the second 'td' element\n",
    "        team_name_tag = row.find_all(\"td\")[1].find(\"a\")\n",
    "        if team_name_tag:\n",
    "            team_name = team_name_tag.get_text(strip=True)\n",
    "        else:\n",
    "            team_name = row.find_all(\"td\")[1].get_text(strip=True)\n",
    "\n",
    "        # Get the correct payroll column (assumed to be in the 7th column)\n",
    "        payroll_column = row.find_all(\"td\")[6].get_text(strip=True)\n",
    "\n",
    "        team_names.append(team_name)\n",
    "        team_payroll.append(payroll_column)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df = pd.DataFrame({\"Team\": team_names, \"Total_Payroll\": team_payroll})\n",
    "\n",
    "    # Save DataFrame to a CSV file\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"team_salary_{year}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping completed. CSV files saved in the 'output' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.safari.service import Service as SafariService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "\n",
    "# Directory to store CSV files\n",
    "OUTPUT_DIR = \"team_records\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Set up the Selenium WebDriver for Safari\n",
    "service = SafariService()\n",
    "driver = webdriver.Safari(service=service)\n",
    "\n",
    "# Base URL to scrape\n",
    "BASE_URL = \"https://www.hockey-reference.com/leagues/NHL_{}.html\"\n",
    "\n",
    "# Years to scrape\n",
    "years = [2016, 2017, 2018]\n",
    "\n",
    "# Columns to extract\n",
    "columns_to_extract = {\n",
    "    \"team_name\": \"Team\",\n",
    "    \"games\": \"GP\",\n",
    "    \"wins\": \"W\",\n",
    "    \"losses\": \"L\",\n",
    "    \"losses_ot\": \"OTL\",\n",
    "    \"points\": \"PTS\",\n",
    "}\n",
    "\n",
    "# XPath to locate specific columns in the table\n",
    "xpaths = {\n",
    "    \"team_name\": './/td[@data-stat=\"team_name\"]/a',\n",
    "    \"games\": './/td[@data-stat=\"games\"]',\n",
    "    \"wins\": './/td[@data-stat=\"wins\"]',\n",
    "    \"losses\": './/td[@data-stat=\"losses\"]',\n",
    "    \"losses_ot\": './/td[@data-stat=\"losses_ot\"]',\n",
    "    \"points\": './/td[@data-stat=\"points\"]',\n",
    "}\n",
    "\n",
    "# Step 2: Create a dictionary mapping full names to abbreviations and team IDs\n",
    "name_to_abbreviation = {\n",
    "    \"Washington Capitals\": (\"WAS\", 15),\n",
    "    \"Dallas Stars\": (\"DAL\", 25),\n",
    "    \"St. Louis Blues\": (\"STL\", 19),\n",
    "    \"Pittsburgh Penguins\": (\"PIT\", 5),\n",
    "    \"Chicago Blackhawks\": (\"CHI\", 16),\n",
    "    \"Florida Panthers\": (\"FLA\", 13),\n",
    "    \"Anaheim Ducks\": (\"ANA\", 24),\n",
    "    \"Los Angeles Kings\": (\"LAK\", 26),\n",
    "    \"New York Rangers\": (\"NYR\", 3),\n",
    "    \"New York Islanders\": (\"NYI\", 2),\n",
    "    \"San Jose Sharks\": (\"SJS\", 28),\n",
    "    \"Tampa Bay Lightning\": (\"TBL\", 14),\n",
    "    \"Nashville Predators\": (\"NSH\", 18),\n",
    "    \"Philadelphia Flyers\": (\"PHI\", 4),\n",
    "    \"Detroit Red Wings\": (\"DET\", 17),\n",
    "    \"Boston Bruins\": (\"BOS\", 6),\n",
    "    \"Minnesota Wild\": (\"MIN\", 30),\n",
    "    \"Carolina Hurricanes\": (\"CAR\", 12),\n",
    "    \"Ottawa Senators\": (\"OTT\", 9),\n",
    "    \"New Jersey Devils\": (\"NJD\", 1),\n",
    "    \"Montreal Canadiens\": (\"MTL\", 8),\n",
    "    \"Colorado Avalanche\": (\"COL\", 21),\n",
    "    \"Buffalo Sabres\": (\"BUF\", 7),\n",
    "    \"Winnipeg Jets\": (\"WPG\", 52),\n",
    "    \"Arizona Coyotes\": (\"ARI\", 53),\n",
    "    \"Calgary Flames\": (\"CGY\", 20),\n",
    "    \"Columbus Blue Jackets\": (\"CBJ\", 29),\n",
    "    \"Vancouver Canucks\": (\"VAN\", 23),\n",
    "    \"Edmonton Oilers\": (\"EDM\", 22),\n",
    "    \"Toronto Maple Leafs\": (\"TOR\", 10),\n",
    "    \"Vegas Golden Knights\": (\"VGK\", 54)\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "team_df = pd.DataFrame.from_dict(\n",
    "    name_to_abbreviation, orient=\"index\", columns=[\"Abbreviation\", \"Team_ID\"]\n",
    ")\n",
    "team_df.reset_index(inplace=True)\n",
    "team_df.rename(columns={\"index\": \"Team_Name\"}, inplace=True)\n",
    "\n",
    "# Display the team DataFrame\n",
    "print(\"Team DataFrame:\")\n",
    "print(team_df)\n",
    "\n",
    "# Loop through each year and scrape the data\n",
    "for year in years:\n",
    "    # Generate the URL for the current year\n",
    "    url = BASE_URL.format(year)\n",
    "\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until the table is loaded\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"stats\"]/tbody/tr'))\n",
    "    )\n",
    "\n",
    "    # Locate the rows in the table\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"stats\"]/tbody/tr')\n",
    "\n",
    "    # Prepare the list to hold the data\n",
    "    data = []\n",
    "\n",
    "        # Iterate over each row and extract the specific columns\n",
    "    for row in rows:\n",
    "        team_data = {}\n",
    "        for key, xpath in xpaths.items():\n",
    "            try:\n",
    "                element = row.find_element(By.XPATH, xpath)\n",
    "                team_data[columns_to_extract[key]] = element.text\n",
    "            except NoSuchElementException:\n",
    "                # If the element is not found, log it and move on\n",
    "                print(f\"Element '{columns_to_extract[key]}' not found for team in year {year}.\")\n",
    "                team_data[columns_to_extract[key]] = \"N/A\"  # Or use another placeholder value\n",
    "        data.append(team_data)\n",
    "\n",
    "    # Convert the data to a DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns_to_extract.values())\n",
    "\n",
    "    # Merge the scraped data with the team_df to include Abbreviation and Team_ID\n",
    "    merged_df = pd.merge(df, team_df, left_on=\"Team\", right_on=\"Team_Name\", how=\"left\")\n",
    "\n",
    "    # Drop the redundant 'Team_Name' column from the merged DataFrame\n",
    "    merged_df.drop(columns=[\"Team_Name\"], inplace=True)\n",
    "\n",
    "    # Save the merged DataFrame to a CSV file for the current year\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"NHL_{year}_team_stats.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Scraping and merging completed for {year}. Data saved to '{output_file}'\")\n",
    "\n",
    "# Close the Safari WebDriver\n",
    "driver.quit()\n",
    "\n",
    "print(\"All seasons scraped, merged, and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from db_utils import get_db_engine, get_metadata\n",
    "\n",
    "engine = get_db_engine()\n",
    "metadata = get_metadata()\n",
    "\n",
    "# Define metadata and tables\n",
    "metadata = MetaData()\n",
    "\n",
    "# Create tables for each season\n",
    "seasons = [\"2016\", \"2017\", \"2018\"]\n",
    "# tables = {season: create_team_data_table(f\"team_data_{season}\") for season in seasons}\n",
    "\n",
    "# Create tables in the database\n",
    "metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "for season in seasons:\n",
    "    try:\n",
    "        # Define the paths to the CSV files for each season\n",
    "        stats_path = f\"team_records/NHL_{int(season)}_team_stats.csv\"\n",
    "        salary_path = f\"team_salaries/team_salary_{int(season) - 1}.csv\"\n",
    "\n",
    "        # Load the stats and team salary data\n",
    "        stats_data = pd.read_csv(stats_path)\n",
    "        salary_data = pd.read_csv(salary_path)\n",
    "\n",
    "        # Merge on different column names\n",
    "        merged_data = pd.merge(\n",
    "            stats_data,\n",
    "            salary_data,\n",
    "            left_on=\"Abbreviation\",\n",
    "            right_on=\"Team\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "        # Drop the redundant 'Team_y' column from the merged DataFrame\n",
    "        merged_data.drop(columns=[\"Team_y\"], inplace=True)\n",
    "\n",
    "        # Display the merged data to verify\n",
    "        print(f\"Merged data for season {season}:\\n\", merged_data.head())\n",
    "\n",
    "        # Insert the merged data into a new table in the database\n",
    "        table_name = f\"merged_team_stats_{season}\"\n",
    "        merged_data.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "        print(\n",
    "            f\"Data for season {season} has been successfully inserted into the database.\"\n",
    "        )\n",
    "\n",
    "        # Delete the CSV files after successful insertion\n",
    "        try:\n",
    "            os.remove(stats_path)\n",
    "            os.remove(salary_path)\n",
    "            print(f\"CSV files for season {season} have been successfully deleted.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error: {e.strerror} - while deleting files for season {season}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process data for season {season}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we scraped player data from Spotrac, inserted the data into a database, and aggregated it for further analysis. These steps are crucial for generating meaningful insights into player performance and salary metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cost_cup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
