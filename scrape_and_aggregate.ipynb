{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping, Inserting, and Aggregating Hockey Data\n",
    "\n",
    "This notebook demonstrates how to scrape player data from Spotrac, insert the scraped data into a database, and then aggregate the data for further analysis.\n",
    "\n",
    "## 1. Setup and Environment Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    BigInteger,\n",
    "    Column,\n",
    "    Float,\n",
    "    Integer,\n",
    "    MetaData,\n",
    "    String,\n",
    "    Table,\n",
    "    create_engine,\n",
    ")\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the database connection\n",
    "DATABASE_TYPE = os.getenv(\"DATABASE_TYPE\")\n",
    "DBAPI = os.getenv(\"DBAPI\")\n",
    "ENDPOINT = os.getenv(\"ENDPOINT\")\n",
    "USER = os.getenv(\"USER\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "PORT = int(os.getenv(\"PORT\", 5432))\n",
    "DATABASE = os.getenv(\"DATABASE\")\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\"\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping Player Data from Spotrac\n",
    "\n",
    "We will scrape player cap hit data for the years 2015, 2016, and 2017 from the Spotrac website using Selenium and BeautifulSoup.\n",
    "The selenium driver is set to safari browser. If you are using a different browser, you will need to adjust the code. \n",
    "driver = webdriver.Safari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Base URL to scrape\n",
    "BASE_URL = \"https://www.spotrac.com/nhl/rankings/player/_/year/{}/sort/cap_total\"\n",
    "\n",
    "# Years to scrape\n",
    "years = [2015, 2016, 2017]\n",
    "\n",
    "# Directory to store CSV files\n",
    "OUTPUT_DIR = \"player_cap_hits\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dictionary to store DataFrames for each year\n",
    "dfs_by_year = {}\n",
    "\n",
    "def split_player_name(name):\n",
    "    \"\"\"Function to clean and split player name\"\"\"\n",
    "    name_parts = name.split()\n",
    "    first_name = name_parts[0]\n",
    "    last_name = \" \".join(name_parts[1:]) if len(name_parts) > 1 else \"\"\n",
    "    return first_name, last_name\n",
    "\n",
    "# Loop through each year and scrape the data\n",
    "for year in years:\n",
    "    url = BASE_URL.format(year)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until the table is loaded\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"list-group-item\"))\n",
    "    )\n",
    "\n",
    "    # Scroll to the bottom of the page to load all content (if applicable)\n",
    "    while True:\n",
    "        previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(2)  # Wait for new data to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == previous_height:\n",
    "            break  # Exit the loop when no more new content is loaded\n",
    "\n",
    "    # Get page source and parse with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find the elements containing the player names and cap hits\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    cap_hits = []\n",
    "\n",
    "    for item in soup.find_all(\"li\", class_=\"list-group-item\"):\n",
    "        name_div = item.find(\"div\", class_=\"link\")\n",
    "        cap_hit_span = item.find(\"span\", class_=\"medium\")\n",
    "        if name_div and cap_hit_span:\n",
    "            name = name_div.text.strip()\n",
    "            first_name, last_name = split_player_name(name)\n",
    "            cap_hit = cap_hit_span.text.strip()\n",
    "            first_names.append(first_name)\n",
    "            last_names.append(last_name)\n",
    "            cap_hits.append(cap_hit)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df = pd.DataFrame(\n",
    "        {\"firstName\": first_names, \"lastName\": last_names, \"capHit\": cap_hits}\n",
    "    )\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    dfs_by_year[year] = df\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrames for each year\n",
    "for year, df in dfs_by_year.items():\n",
    "    print(f\"Data for {year}:\")\n",
    "    display(df.head())\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "for year, df in dfs_by_year.items():\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"player_cap_hits_{year}.csv\")\n",
    "    df.to_csv(csv_path, index=False, mode=\"w\")\n",
    "\n",
    "print(f\"Data saved to {OUTPUT_DIR} directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inserting Scraped Data into the Database\n",
    "\n",
    "Next, we will insert the scraped data from the CSV files into the `hockey_stats` database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create cap hit tables\n",
    "def create_caphit_table(table_name):\n",
    "    \"\"\"Define table creation function to avoid repetition\"\"\"\n",
    "    return Table(\n",
    "        table_name,\n",
    "        metadata,\n",
    "        Column(\"firstName\", String(50)),\n",
    "        Column(\"lastName\", String(50)),\n",
    "        Column(\"capHit\", String(50)),\n",
    "    )\n",
    "\n",
    "# Create tables for each season\n",
    "metadata = MetaData()\n",
    "seasons = [\"20152016\", \"20162017\", \"20172018\"]\n",
    "tables = {season: create_caphit_table(f\"player_cap_hit_{season}\") for season in seasons}\n",
    "metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "def insert_data_from_csv(engine, table_name, file_path):\n",
    "    \"\"\"Insert data from CSV into the specified database table.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.to_sql(table_name, con=engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"Data inserted successfully into {table_name}\")\n",
    "\n",
    "        # Remove the file after successful insertion\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} deleted successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {file_path} - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing file '{file_path}': {e}\")\n",
    "\n",
    "# Define directories and mappings for insertion\n",
    "csv_files_and_mappings = [\n",
    "    (\"player_cap_hits/player_cap_hits_2015.csv\", \"player_cap_hit_20152016\"),\n",
    "    (\"player_cap_hits/player_cap_hits_2016.csv\", \"player_cap_hit_20162017\"),\n",
    "    (\"player_cap_hits/player_cap_hits_2017.csv\", \"player_cap_hit_20172018\"),\n",
    "]\n",
    "\n",
    "# Insert data into database tables\n",
    "with Session() as session:\n",
    "    for file_path, table_name in csv_files_and_mappings:\n",
    "        insert_data_from_csv(engine, table_name, file_path)\n",
    "\n",
    "    print(\"Data inserted successfully into all tables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate Data and insert into the Database\n",
    "\n",
    "Finally, we will aggregate the data per season per player and insert the results into an aggregated table in the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get data from the database\n",
    "def get_data_from_db(query):\n",
    "    \"\"\"Function to get data from the database.\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        return pd.read_sql(query, connection)\n",
    "\n",
    "def create_aggregated_table(table_name):\n",
    "    \"\"\"Create table schema for aggregated table.\"\"\"\n",
    "    metadata = MetaData()\n",
    "    Table(\n",
    "        table_name,\n",
    "        metadata,\n",
    "        Column(\"player_id\", BigInteger, primary_key=True),\n",
    "        Column(\"firstName\", String),\n",
    "        Column(\"lastName\", String),\n",
    "        Column(\"corsi_for\", Float),\n",
    "        Column(\"corsi_against\", Float),\n",
    "        Column(\"corsi\", Float),\n",
    "        Column(\"CF_Percent\", Float),\n",
    "        Column(\"timeOnIce\", Float),\n",
    "        Column(\"game_count\", Integer),\n",
    "        Column(\"Cap_Hit\", Float),\n",
    "    )\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "# Loop through each season to aggregate data\n",
    "for season in [\"20152016\", \"20162017\", \"20172018\"]:\n",
    "    CORSI_QUERY = f\"SELECT * FROM raw_corsi_{season}\"\n",
    "    df_corsi = get_data_from_db(CORSI_QUERY)\n",
    "    if \"Unnamed: 0\" in df_corsi.columns:\n",
    "        df_corsi = df_corsi.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    GSS_TOI_QUERY = 'SELECT game_id, player_id, \"timeOnIce\" FROM game_skater_stats'\n",
    "    df_gss_toi = get_data_from_db(GSS_TOI_QUERY)\n",
    "\n",
    "    PLAYER_INFO_QUERY = (\n",
    "        'SELECT player_id, \"firstName\", \"lastName\", \"primaryPosition\" FROM player_info'\n",
    "    )\n",
    "    df_player_info = get_data_from_db(PLAYER_INFO_QUERY)\n",
    "\n",
    "    # Merge dataframes\n",
    "    df_all = pd.merge(df_corsi, df_gss_toi, on=[\"game_id\", \"player_id\"])\n",
    "    df_all = pd.merge(df_all, df_player_info, on=\"player_id\")\n",
    "\n",
    "    # Group and aggregate player stats\n",
    "    df_grouped_all = (\n",
    "        df_all.groupby(\"player_id\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"firstName\": \"first\",\n",
    "                \"lastName\": \"first\",\n",
    "                \"corsi_for\": \"mean\",\n",
    "                \"corsi_against\": \"mean\",\n",
    "                \"corsi\": \"mean\",\n",
    "                \"CF_Percent\": \"mean\",\n",
    "                \"timeOnIce\": \"mean\",\n",
    "                \"game_id\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"game_id\": \"game_count\"})\n",
    "    )\n",
    "\n",
    "    PLAYER_SALARY_QUERY = (\n",
    "        f'SELECT \"firstName\", \"lastName\", \"capHit\" FROM player_cap_hit_{season}'\n",
    "    )\n",
    "    df_player_salary = get_data_from_db(PLAYER_SALARY_QUERY)\n",
    "    print(df_player_salary.head())\n",
    "\n",
    "    # Convert capHit from string to float\n",
    "    df_player_salary[\"capHit\"] = (\n",
    "        df_player_salary[\"capHit\"].replace(r\"[\\$,]\", \"\", regex=True).astype(float)\n",
    "    )\n",
    "\n",
    "    # Merge aggregated stats with salary info\n",
    "    df_grouped_all = pd.merge(\n",
    "        df_grouped_all, df_player_salary, on=[\"firstName\", \"lastName\"]\n",
    "    )\n",
    "\n",
    "    # Post-processing\n",
    "    df_grouped_all[\"CF_Percent\"] = (df_grouped_all[\"CF_Percent\"].round(4) * 100).round(4)\n",
    "    df_grouped_all[\"timeOnIce\"] = df_grouped_all[\"timeOnIce\"].round(2)\n",
    "\n",
    "    THRESHOLD = 82 * 0.32\n",
    "    df_grouped_all = df_grouped_all.query(f\"game_count >= {THRESHOLD}\")\n",
    "\n",
    "    df_grouped_all[\"CF_Percent\"] = df_grouped_all[\"CF_Percent\"].apply(\n",
    "        lambda x: np.round(x, 4)\n",
    "    )\n",
    "    df_grouped_all[\"timeOnIce\"] = df_grouped_all[\"timeOnIce\"].apply(\n",
    "        lambda x: np.round(x, 2)\n",
    "    )\n",
    "\n",
    "    df_grouped_all = df_grouped_all.sort_values(\"CF_Percent\", ascending=False)\n",
    "\n",
    "    aggregated_table_name = f\"aggregated_corsi_{season}\"\n",
    "    create_aggregated_table(aggregated_table_name)\n",
    "\n",
    "    # Insert aggregated data into the new table\n",
    "    df_grouped_all.to_sql(\n",
    "        aggregated_table_name, con=engine, if_exists=\"replace\", index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Data inserted successfully into {aggregated_table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we scraped player data from Spotrac, inserted the data into a database, and aggregated it for further analysis. These steps are crucial for generating meaningful insights into player performance and salary metrics.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
